{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Task 3 - Optimizers\n",
    "\n",
    "**Requirements:**\n",
    " - numpy (https://numpy.org/)\n",
    " - matplotlib (https://matplotlib.org/)\n",
    "or\n",
    " - Plotly (https://plotly.com/)\n",
    "\n",
    "Let's continue with our framework. We use all of the previous implemented classes (with some modifications) and add new - **Optimizers**.\n",
    "\n",
    "Watch out for the shape of input data.. Now we are working with mini-batches $(B,nX,1)$, where $B$ is number of samples in mini-batch, $nX$ is number of features and $1$ is for vector/matrix multiplication in the last 2 dimensions, leaving $B$ as samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "from utils import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Linear layer (Dense, Fully connected, Single Layer Perceptron)\n",
    "#------------------------------------------------------------------------------\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.b = np.zeros((out_features, 1))\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.aPred = input\n",
    "        self.m = self.aPred.shape[0]\n",
    "        net = np.matmul(self.W, input) + self.b\n",
    "        return net\n",
    "\n",
    "    def backward(self, dz: np.ndarray) -> np.ndarray:\n",
    "        self.dW = (1.0/self.m) * np.sum(np.matmul(dz, self.aPred.transpose((0,2,1))), axis=0)\n",
    "        self.db = (1.0/self.m) * np.sum(dz, axis=0)\n",
    "        return np.matmul(self.W.transpose(), dz)\n",
    "\n",
    "    def get_optimizer_context(self):\n",
    "        return [[self.W, self.dW], [self.b, self.db]]\n",
    "\n",
    "    def update_parameters(self, params):\n",
    "        self.W, self.b = params\n",
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Sigmoid(Module):\n",
    "    def __init__(self):\n",
    "        super(Sigmoid, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return 1.0 / (1.0 + np.exp(-input))\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, np.multiply(a, 1 - a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   HyperbolicTangentActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class Tanh(Module):\n",
    "    def __init__(self):\n",
    "        super(Tanh, self).__init__()\n",
    "\n",
    "    def forward(self, input: np.ndarray) -> np.ndarray:\n",
    "        self.fw_input = input\n",
    "        return (np.exp(2 * input) - 1) / (np.exp(2 * input) + 1)\n",
    "\n",
    "    def backward(self, da) -> np.ndarray:\n",
    "        a = self(self.fw_input)\n",
    "        return np.multiply(da, 1 - np.square(a))\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model(Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "    def forward(self, input) -> np.ndarray:\n",
    "        for name, module in self.modules.items():\n",
    "            input = module(input)\n",
    "        return input\n",
    "\n",
    "    def backward(self, z: np.ndarray):\n",
    "        for name, module in reversed(self.modules.items()):\n",
    "            z = module.backward(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Loss Functions\n",
    "\n",
    "As in standard deep learning frameworks, calling Loss function can return either **cost** or  **loss**  based on parameter **reduce**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   MeanSquareErrorLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class MSELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(MSELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(np.mean(np.power(target - input, 2), axis=0, keepdims=True))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return np.mean(-2 * (target - input), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BCELoss(Module):\n",
    "    def __init__(self, reduce=\"mean\"):\n",
    "        super(BCELoss, self).__init__()\n",
    "        if reduce == \"mean\":\n",
    "            self.reduce_fn = np.mean\n",
    "        elif reduce == \"sum\":\n",
    "            self.reduce_fn = np.sum\n",
    "        elif reduce is None:\n",
    "            # return identity (do nothing)\n",
    "            self.reduce_fn = lambda x : x\n",
    "        else:\n",
    "            raise AttributeError\n",
    "\n",
    "    def forward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return self.reduce_fn(-(target * np.log(input) + np.multiply((1 - target), np.log(1 - input))))\n",
    "\n",
    "    def backward(self, input: np.ndarray, target: np.ndarray) -> np.ndarray:\n",
    "        return -np.divide(target, input) + np.divide(1 - target, 1 - input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Optimizers\n",
    "\n",
    "Each optimizer gets as input a **model** and loads each layer's parameters for optimizer context **`layer.get_optimizer_context()`**. Other attributes are based on the optimizer definition. The modified parameters are put back to the model's layer by `layer.set_optimizer_context([W,b])`. Remember that optimizers may require to store some context for the next steps of optimization for each layer and each parameter accordingly.\n",
    "\n",
    "Your task is to implement:\n",
    " - SGD with momentum\n",
    " - RMSProp: http://www.cs.toronto.edu/~hinton/coursera/lecture6/lec6.pdf\n",
    " - Adam: https://arxiv.org/pdf/1412.6980.pdf\n",
    "\n",
    "All algorithms are in [https://www.deeplearningbook.org/contents/optimization.html](https://www.deeplearningbook.org/contents/optimization.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   AbstractOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Optimizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplemented\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   StochasticGradientDescentOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, model:Model, lr:float):\n",
    "        super(SGD, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.context = {}\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "                    # >>>> start here\n",
    "                    W = W - self.lr * dW\n",
    "                    b = b - self.lr * db\n",
    "                    # <<<< end here\n",
    "                    layer.update_parameters([W,b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   SGDMomentumOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(self, model, lr:float, momentum):\n",
    "        super(SGDMomentum, self).__init__()\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW],[b,db]] = params\n",
    "                    if name in self.context.keys():\n",
    "                    # >>>> start_solution\n",
    "                    # self.context[name][???] =\n",
    "                        self.context[name]['vW'] = np.multiply(self.context[name]['vW'], self.momentum) - np.multiply(self.lr, dW)\n",
    "                        self.context[name]['vb'] = np.multiply(self.context[name]['vb'], self.momentum) - np.multiply(self.lr, db)\n",
    "                        pass\n",
    "                    else:\n",
    "                        self.context[name] = {}\n",
    "                        self.context[name]['vW'] = np.multiply(-self.lr, dW)\n",
    "                        self.context[name]['vb'] = np.multiply(-self.lr, db)\n",
    "                        pass\n",
    "                    W += self.context[name]['vW'] \n",
    "                    b += self.context[name]['vb'] \n",
    "                    # <<<< end_solution\n",
    "                    layer.update_parameters([W,b])\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RMSpropOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class RMSprop(Optimizer):\n",
    "    def __init__(self, model, lr:float, dec: float, eps=1e-07):\n",
    "        super(RMSprop, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "        self.lr = lr\n",
    "        self.dec = dec\n",
    "        self.eps = eps\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "                    if name in self.context.keys():\n",
    "                    # >>>> start_solution\n",
    "                    #    self.context[name][???] =\n",
    "                        self.context[name]['sdW'] = np.multiply((1-self.dec),np.power(dW, 2)) + self.dec*self.context[name]['sdW']\n",
    "                        self.context[name]['sdB'] = np.multiply((1-self.dec),np.power(db, 2)) + self.dec*self.context[name]['sdB']\n",
    "                    else:\n",
    "                        self.context[name] = {}\n",
    "                        self.context[name]['sdW'] = np.multiply((1-self.dec),np.power(dW, 2))\n",
    "                        self.context[name]['sdB'] = np.multiply((1-self.dec),np.power(db, 2))\n",
    "                    # <<<< end_solution\n",
    "\n",
    "                    W -= self.lr * np.divide(dW, np.sqrt(self.eps + self.context[name]['sdW'])) \n",
    "                    b -= self.lr * np.divide(db, np.sqrt(self.eps + self.context[name]['sdB']))\n",
    "\n",
    "                    layer.update_parameters([W, b])\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   AdamOptimizer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Adam(Optimizer):\n",
    "    def __init__(self, model, lr, dec1, dec2, eps:float = 0.001, delt:float = 1e-08):\n",
    "        super(Adam, self).__init__()\n",
    "        self.model = model\n",
    "        self.context = {}\n",
    "        # >>>> start_solution\n",
    "        self.lr = lr\n",
    "        self.dec1 = dec1\n",
    "        self.dec2 = dec2\n",
    "        self.eps = eps\n",
    "        # <<<< end_solution\n",
    "\n",
    "    def step(self):\n",
    "        # >>>>>> Probably add something here ;)\n",
    "        t = 0\n",
    "        s = 0\n",
    "        r = 0\n",
    "        # <<<<<< until here\n",
    "        for name, layer in self.model.modules.items():\n",
    "            if hasattr(layer, 'get_optimizer_context'):\n",
    "                params = layer.get_optimizer_context()\n",
    "                if params is not None:\n",
    "                    [[W, dW], [b, db]] = params\n",
    "                    if name in self.context.keys():\n",
    "                        # >>>> start_solution\n",
    "                        #self.context[name][???] =\n",
    "\n",
    "                        t += 1\n",
    "                    else:\n",
    "                        self.context[name] = {}\n",
    "                        t += 1\n",
    "                        self.context[name]['sdW'] = (1-self.dec1) * dW\n",
    "                        self.context[name]['sdb'] = (1-self.dec1) * db\n",
    "\n",
    "                        self.context[name]['rdW'] = (1-self.dec2) * np.power(dW,2)\n",
    "                        self.context[name]['rdb'] = (1-self.dec2) * np.power(db,2) \n",
    "                    # <<<< end_solution\n",
    "\n",
    "                    layer.update_parameters([W, b])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Main Processing Cell\n",
    "\n",
    "Watch out for the shape of mini-batch (B,Features,1)\n",
    "\n",
    " 1. Initialize dataset (`dataset_Flower`).\n",
    " 2. Declare a simple model.\n",
    " 3. Initialize optimizer.\n",
    " 4. Make mini-batches.\n",
    " 5. Perform forward pass through the network.\n",
    " 6. Compute loss.\n",
    " 7. Backward prop loss.\n",
    " 8. Track loss.\n",
    " 9. Backward pass MLP.\n",
    " 10. Use optimizer to modify model parameters.\n",
    " 11. Repeat for $N$ epochs\n",
    " 12. Visualize other plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from utils import gradient_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dataset import dataset_Flower, MakeBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = MakeBatches(dataset_Flower(m=512, noise=0.3), 32, True)\n",
    "###>>> start of solution\n",
    "mlp = Model()\n",
    "mlp.add_module(Linear(2, 3), 'Dense_1')\n",
    "mlp.add_module(Tanh(), 'Tanh_1')\n",
    "mlp.add_module(Linear(3, 4), 'Dense_2')\n",
    "mlp.add_module(Tanh(), 'Tanh_2')\n",
    "mlp.add_module(Linear(4, 5), 'Dense_3')\n",
    "mlp.add_module(Tanh(), 'Tanh_3')\n",
    "mlp.add_module(Linear(5, 1), 'Dense_4_out')\n",
    "mlp.add_module(Sigmoid(), 'Sigmoid')\n",
    "loss_fn = MSELoss(reduce='mean')\n",
    "\n",
    "#optimizer = SGD(mlp, lr=0.001)\n",
    "#optimizer = SGDMomentum(mlp, lr=0.001, momentum=0.5)\n",
    "optimizer = RMSprop(mlp, lr=0.001, dec=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "Epoch  100\n",
      "Epoch  200\n",
      "Epoch  300\n",
      "Epoch  400\n",
      "Epoch  500\n",
      "Epoch  600\n",
      "Epoch  700\n",
      "Epoch  800\n",
      "Epoch  900\n"
     ]
    }
   ],
   "source": [
    "N_epochs = 1000\n",
    "losses = []\n",
    "for i in range(N_epochs):\n",
    "    if(i%100==0):\n",
    "        print('Epoch ',i)\n",
    "    epoch_loss = []\n",
    "    for mini_batch_X, mini_batch_Y in dataset:\n",
    "        predicted_Y_hat = mlp.forward(mini_batch_X)\n",
    "        loss = loss_fn(predicted_Y_hat, mini_batch_Y)\n",
    "        epoch_loss += [np.mean(loss)]\n",
    "        dLoss = loss_fn.backward(predicted_Y_hat, mini_batch_Y)\n",
    "        mlp.backward(dLoss)\n",
    "        # gradient_check(mlp, loss_fn, mini_batch_X, mini_batch_Y)\n",
    "        optimizer.step()\n",
    "    losses += [np.mean(epoch_loss)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=SGD<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "SGD",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "SGD",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899,
          900,
          901,
          902,
          903,
          904,
          905,
          906,
          907,
          908,
          909,
          910,
          911,
          912,
          913,
          914,
          915,
          916,
          917,
          918,
          919,
          920,
          921,
          922,
          923,
          924,
          925,
          926,
          927,
          928,
          929,
          930,
          931,
          932,
          933,
          934,
          935,
          936,
          937,
          938,
          939,
          940,
          941,
          942,
          943,
          944,
          945,
          946,
          947,
          948,
          949,
          950,
          951,
          952,
          953,
          954,
          955,
          956,
          957,
          958,
          959,
          960,
          961,
          962,
          963,
          964,
          965,
          966,
          967,
          968,
          969,
          970,
          971,
          972,
          973,
          974,
          975,
          976,
          977,
          978,
          979,
          980,
          981,
          982,
          983,
          984,
          985,
          986,
          987,
          988,
          989,
          990,
          991,
          992,
          993,
          994,
          995,
          996,
          997,
          998,
          999
         ],
         "xaxis": "x",
         "y": [
          0.27673489822216196,
          0.2680632966255023,
          0.260478122641848,
          0.25409398279348683,
          0.24899832246703704,
          0.2451697783440349,
          0.24239829092606605,
          0.24041975621869657,
          0.23899162739423216,
          0.23793207723129103,
          0.23710665721662672,
          0.23642352533591182,
          0.23582971301041533,
          0.2352937071078804,
          0.2347940751931721,
          0.23431546340201553,
          0.2338469358555637,
          0.23337981300679922,
          0.23290731216454796,
          0.23242486317025923,
          0.2319294880447001,
          0.23141926179782507,
          0.2308927013328395,
          0.23034787292348727,
          0.2297826937535349,
          0.229196723533882,
          0.2285908522732492,
          0.22796581564567076,
          0.22732186141916794,
          0.22665899607166057,
          0.22597715836468588,
          0.22527630099318108,
          0.224556438292913,
          0.2238176956620841,
          0.22306036207234475,
          0.222284918481245,
          0.221492018191298,
          0.2206824286665728,
          0.2198569681701248,
          0.21901646238407843,
          0.218161724524861,
          0.21729355452389315,
          0.2164127798046451,
          0.21552040432578548,
          0.214617861990195,
          0.21370717062943906,
          0.2127907953377764,
          0.21187133386988258,
          0.2109512599711697,
          0.2100328014415127,
          0.20911790912901776,
          0.2082082701091049,
          0.20730534683242258,
          0.20641043218657495,
          0.20552470603900663,
          0.20464928041703534,
          0.20378522559276774,
          0.2029335707510048,
          0.20209527534805108,
          0.20127118125586962,
          0.20046197703120539,
          0.19966820463488694,
          0.1988902998435385,
          0.1981286124926647,
          0.19738334895822424,
          0.1966544585460403,
          0.1959415925582497,
          0.19524420227376604,
          0.19456166111116396,
          0.19389330810956223,
          0.1932384572491594,
          0.19259643518763064,
          0.19196662701646877,
          0.1913484919468362,
          0.1907415497293569,
          0.19014535695289353,
          0.18955948742124537,
          0.18898352279694636,
          0.1884170538053564,
          0.1878596877854644,
          0.1873110563998726,
          0.18677081924427422,
          0.1862386633129468,
          0.18571430127307506,
          0.18519747145047755,
          0.18468794034936947,
          0.18418550644518178,
          0.1836900030792656,
          0.18320129864518625,
          0.1827192934272738,
          0.18224391375781482,
          0.18177510499765737,
          0.18131282498831747,
          0.1808570392468848,
          0.18040771859338026,
          0.1799648392969796,
          0.17952838513452224,
          0.1790983496574741,
          0.178674735144499,
          0.17825754291504778,
          0.1778467515394862,
          0.17744228942070933,
          0.17704401978209003,
          0.17665175092925395,
          0.17626526359346983,
          0.17588433666997763,
          0.17550876130392098,
          0.17513834448279064,
          0.17477290695670117,
          0.17441227915284133,
          0.17405629699962832,
          0.17370479865509386,
          0.17335762282311154,
          0.17301460899772156,
          0.17267559925280956,
          0.17234044047747518,
          0.17200898598562028,
          0.17168109625228511,
          0.1713566392944259,
          0.17103549122787057,
          0.17071753701539744,
          0.1704026710113985,
          0.17009079691058712,
          0.16978182698595357,
          0.1694756807894599,
          0.16917228362534742,
          0.1688715650673399,
          0.16857345764928,
          0.16827789574618784,
          0.16798481473263158,
          0.16769415078283378,
          0.16740584183604001,
          0.16711982983113471,
          0.16683606357077607,
          0.16655450142797493,
          0.1662751136435367,
          0.16599788430932255,
          0.1657228130042895,
          0.1654499159036057,
          0.16517922625036421,
          0.1649107942143817,
          0.16464468621845146,
          0.16438098379574745,
          0.16411978201552807,
          0.1638611875111899,
          0.16360531616233903,
          0.16335229050626263,
          0.1631022369733445,
          0.1628552830513886,
          0.16261155448505696,
          0.162371172609504,
          0.16213425190288838,
          0.16190089782216827,
          0.1616712049624967,
          0.1614452555555674,
          0.16122311830022218,
          0.16100484750372274,
          0.1607904825080205,
          0.16058004738374232,
          0.1603735508930096,
          0.16017098674263616,
          0.15997233415920759,
          0.1597775588052073,
          0.15958661401774427,
          0.15939944229987602,
          0.15921597695093703,
          0.15903614370662472,
          0.15885986227760218,
          0.15868704771647774,
          0.15851761158895097,
          0.15835146296068617,
          0.1581885092309529,
          0.15802865684886622,
          0.1578718119433698,
          0.15771788088920208,
          0.1575667708216632,
          0.15741839010509573,
          0.15727264875440455,
          0.1571294588057805,
          0.15698873463174057,
          0.1568503931961904,
          0.15671435424687696,
          0.15658054044481384,
          0.15644887743258035,
          0.15631929384550455,
          0.15619172127146663,
          0.1560660941663615,
          0.15594234973321167,
          0.15582042777367805,
          0.15570027052149585,
          0.15558182246838548,
          0.1554650301944427,
          0.15534984221694267,
          0.15523620887367334,
          0.1551240822586482,
          0.1550134162280237,
          0.15490416649031585,
          0.154796290785479,
          0.15468974914094624,
          0.15458450417088077,
          0.15448052136316762,
          0.15437776928590718,
          0.15427621964969396,
          0.15417584718616828,
          0.15407662934042507,
          0.1539785458114392,
          0.15388157799799407,
          0.15378570841232794,
          0.15369092011329843,
          0.15359719619386045,
          0.15350451934238726,
          0.15341287148855662,
          0.1533222335424294,
          0.15323258523680935,
          0.15314390508334297,
          0.15305617044827566,
          0.1529693577433251,
          0.15288344271326165,
          0.15279840078917345,
          0.15271420746937808,
          0.1526308386903886,
          0.1525482711571755,
          0.1524664826121943,
          0.1523854520330083,
          0.15230515975646022,
          0.15222558753225512,
          0.15214671851079337,
          0.15206853717028979,
          0.15199102918831114,
          0.1519141812646403,
          0.1518379809072893,
          0.15176241620190922,
          0.15168747559521817,
          0.1516131477314417,
          0.15153942138164364,
          0.15146628549489882,
          0.15139372937774012,
          0.1513217429803847,
          0.1512503172444444,
          0.15117944445458606,
          0.15110911853711634,
          0.15103933525701504,
          0.15097009227506264,
          0.15090138903469186,
          0.15083322645526087,
          0.1507656064200163,
          0.1506985310696893,
          0.1506320019495961,
          0.1505660191040432,
          0.15050058025093968,
          0.15043568018007156,
          0.15037131048533092,
          0.15030745966881512,
          0.15024411356926015,
          0.1501812560025924,
          0.15011886947993514,
          0.1500569358871265,
          0.14999543705186802,
          0.1499343551692047,
          0.14987367308958355,
          0.14981337449227738,
          0.1497534439729544,
          0.14969386707236704,
          0.14963463026779872,
          0.14957572094289007,
          0.1495171273461971,
          0.1494588385448198,
          0.14940084437665463,
          0.14934313540304317,
          0.14928570286254067,
          0.14922853862596724,
          0.1491716351526585,
          0.14911498544776464,
          0.1490585830204874,
          0.1490024218432317,
          0.14894649631176005,
          0.1488908012065497,
          0.1488353316556546,
          0.1487800830994504,
          0.14872505125769184,
          0.14867023209931882,
          0.14861562181541826,
          0.14856121679567477,
          0.1485070136085353,
          0.14845300898517824,
          0.14839919980722865,
          0.1483455830980159,
          0.14829215601704004,
          0.14823891585720966,
          0.14818586004434778,
          0.14813298613843157,
          0.14808029183603819,
          0.14802777497350617,
          0.1479754335303786,
          0.14792326563277064,
          0.14787126955638055,
          0.14781944372894149,
          0.14776778673198443,
          0.14771629730184255,
          0.1476649743298799,
          0.1476138168619637,
          0.14756282409723043,
          0.14751199538621057,
          0.14746133022839059,
          0.14741082826928992,
          0.14736048929713447,
          0.14731031323919877,
          0.14726030015788613,
          0.14721045024660492,
          0.14716076382549337,
          0.1471112413370349,
          0.1470618833415992,
          0.14701269051293614,
          0.1469636636336429,
          0.14691480359062006,
          0.14686611137052513,
          0.14681758805522901,
          0.14676923481727705,
          0.1467210529153511,
          0.14667304368972953,
          0.14662520855773556,
          0.14657754900916556,
          0.14653006660168472,
          0.14648276295617702,
          0.1464356397520345,
          0.14638869872236948,
          0.14634194164913275,
          0.1462953703581188,
          0.14624898671384032,
          0.1462027926142519,
          0.1461567899853038,
          0.1461109807753072,
          0.14606536694909183,
          0.14601995048193814,
          0.14597473335326835,
          0.14592971754008055,
          0.14588490501011378,
          0.14584029771473236,
          0.14579589758152234,
          0.14575170650659391,
          0.14570772634658824,
          0.14566395891038786,
          0.14562040595053682,
          0.1455770691543753,
          0.1455339501349006,
          0.1454910504213666,
          0.14544837144963685,
          0.1454059145523101,
          0.14536368094863625,
          0.1453216717342444,
          0.14527988787070273,
          0.1452383301749322,
          0.14519699930849256,
          0.1451558957667596,
          0.1451150198680078,
          0.14507437174241095,
          0.1450339513209672,
          0.14499375832435,
          0.14495379225168034,
          0.144914052369208,
          0.14487453769887987,
          0.1448352470067658,
          0.14479617879129988,
          0.1447573312712831,
          0.14471870237358023,
          0.14468028972042843,
          0.14464209061625652,
          0.14460410203389595,
          0.14456632060004054,
          0.14452874257978687,
          0.14449136386005695,
          0.1444541799316697,
          0.1444171858697852,
          0.14438037631239747,
          0.14434374543649112,
          0.14430728693140554,
          0.14427099396886464,
          0.14423485916902348,
          0.14419887456175656,
          0.1441630315422547,
          0.14412732081981036,
          0.14409173235843825,
          0.14405625530770474,
          0.14402087792180784,
          0.14398558746456566,
          0.1439503700975313,
          0.1439152107479738,
          0.14388009295297996,
          0.14384499867551953,
          0.14380990808810692,
          0.14377479931994128,
          0.1437396481645181,
          0.14370442774736575,
          0.14366910815880005,
          0.14363365606593675,
          0.14359803433366483,
          0.14356220170813416,
          0.14352611265021903,
          0.14348971744948197,
          0.1434529627943306,
          0.14341579300337498,
          0.14337815210433819,
          0.14333998683906823,
          0.143301250448074,
          0.14326190677009598,
          0.143221933891847,
          0.14318132647459506,
          0.14314009610300957,
          0.14309826951803856,
          0.14305588519054183,
          0.14301298908950325,
          0.14296963054781606,
          0.14292585888154818,
          0.1428817210521609,
          0.14283726034495403,
          0.14279251585357705,
          0.1427475225062857,
          0.14270231139826944,
          0.1426569102567517,
          0.14261134392904679,
          0.14256563483366583,
          0.14251980334811531,
          0.14247386812695917,
          0.1424278463541535,
          0.14238175393823826,
          0.1423356056602694,
          0.14228941528402791,
          0.1422431956369862,
          0.1421969586692354,
          0.1421507154963247,
          0.1421044764308219,
          0.14205825100640837,
          0.1420120479974642,
          0.14196587543636655,
          0.14191974063010004,
          0.14187365017724857,
          0.14182760998599364,
          0.1417816252933785,
          0.14173570068580588,
          0.14168984012051616,
          0.14164404694764401,
          0.14159832393236071,
          0.14155267327658783,
          0.14150709663978955,
          0.14146159515842244,
          0.14141616946372154,
          0.1413708196976245,
          0.1413255455267609,
          0.14128034615456128,
          0.14123522033164299,
          0.141190166364717,
          0.1411451821243085,
          0.14110026505160775,
          0.14105541216475576,
          0.14101062006483256,
          0.1409658849417592,
          0.1409212025802562,
          0.1408765683659281,
          0.1408319772914779,
          0.1407874239630001,
          0.1407429026062656,
          0.14069840707289855,
          0.14065393084635514,
          0.1406094670476521,
          0.14056500844085107,
          0.14052054743838716,
          0.14047607610642784,
          0.1404315861705665,
          0.14038706902227724,
          0.1403425157266986,
          0.14029791703245656,
          0.14025326338438765,
          0.1402085449401807,
          0.1401637515921095,
          0.14011887299517786,
          0.14007389860312475,
          0.1400288177138156,
          0.13998361952554306,
          0.13993829320562223,
          0.13989282797233538,
          0.1398472131906962,
          0.13980143848161256,
          0.13975549384282,
          0.13970936977848303,
          0.13966305743274926,
          0.13961654872102197,
          0.13956983645157822,
          0.13952291442971884,
          0.13947577753712795,
          0.13942842178063458,
          0.13938084430697434,
          0.13933304338310354,
          0.13928501834463936,
          0.13923676951758807,
          0.13918829812029057,
          0.139139606153274,
          0.13909069628448392,
          0.13904157173639142,
          0.13899223618002607,
          0.13894269363938172,
          0.13889294840812832,
          0.13884300497929342,
          0.1387928679876253,
          0.1387425421637163,
          0.1386920322986034,
          0.13864134321741825,
          0.13859047976065844,
          0.13853944677174654,
          0.13848824908968013,
          0.13843689154573344,
          0.13838537896332126,
          0.13833371616027196,
          0.13828190795287248,
          0.13822995916114283,
          0.13817787461487538,
          0.13812565916003586,
          0.1380733176651719,
          0.13802085502751685,
          0.13796827617851237,
          0.13791558608850638,
          0.1378627897704141,
          0.1378098922821639,
          0.1377568987277797,
          0.13770381425698863,
          0.13765064406327643,
          0.13759739338034752,
          0.1375440674769847,
          0.13749067165033613,
          0.1374372112176906,
          0.13738369150683538,
          0.1373301178451171,
          0.13727649554735405,
          0.13722282990277251,
          0.13716912616115734,
          0.13711538951843028,
          0.13706162510188002,
          0.13700783795528562,
          0.13695403302417913,
          0.13690021514149536,
          0.13684638901384155,
          0.13679255920857708,
          0.1367387301418106,
          0.13668490606727263,
          0.1366310910658059,
          0.1365772890349528,
          0.13652350367794225,
          0.13646973849153252,
          0.13641599675298782,
          0.13636228150803043,
          0.13630859556320502,
          0.13625494148611442,
          0.13620132161429258,
          0.13614773806938396,
          0.13609419277110327,
          0.13604068744690584,
          0.13598722363671295,
          0.13593380269466393,
          0.13588042579058413,
          0.13582709391318026,
          0.13577380787581408,
          0.13572056832467774,
          0.13566737574853566,
          0.1356142304888875,
          0.1355611327493358,
          0.13550808260300198,
          0.13545507999696782,
          0.13540212475291097,
          0.13534921656337978,
          0.13529635498354675,
          0.13524353941884854,
          0.1351907691097109,
          0.1351380431156013,
          0.13508536030197033,
          0.1350327193351839,
          0.1349801186922004,
          0.13492755669325118,
          0.1348750315667384,
          0.1348225415553584,
          0.1347700850703354,
          0.134717660895801,
          0.13466526843721682,
          0.13461290799637948,
          0.13456058104209728,
          0.1345082904324728,
          0.13445604053507929,
          0.1344038371880793,
          0.13435168744933546,
          0.13429959908921218,
          0.13424757979003907,
          0.13419563601392975,
          0.13414377148758178,
          0.13409198523806876,
          0.1340402691411025,
          0.13398860513039584,
          0.13393696280990458,
          0.1338852995212756,
          0.13383356664851492,
          0.13378172545953915,
          0.13372976850141213,
          0.13367773188916865,
          0.1336256870607845,
          0.13357371952993444,
          0.13352191064681695,
          0.1334703291539065,
          0.1334190300868702,
          0.13336805683070552,
          0.13331744377746485,
          0.13326721859061397,
          0.1332174038713371,
          0.13316801830415623,
          0.1331190774166533,
          0.13307059407120822,
          0.13302257877550033,
          0.13297503987150694,
          0.132927983642964,
          0.13288141436744766,
          0.13283533432944747,
          0.132789743803441,
          0.13274464100991906,
          0.1327000220417806,
          0.132655880753102,
          0.1326122085970105,
          0.13256899439496656,
          0.13252622401795752,
          0.13248387996394972,
          0.13244194082933325,
          0.1324003806981232,
          0.13235916851042173,
          0.13231826751255096,
          0.13227763491843542,
          0.13223722190577808,
          0.13219697402106423,
          0.13215683198348926,
          0.1321167327868563,
          0.1320766109325458,
          0.13203639960722574,
          0.1319960316490254,
          0.13195544021202738,
          0.13191455911834304,
          0.13187332295589554,
          0.131831667021508,
          0.13178952721804496,
          0.13174683999727577,
          0.13170354240848967,
          0.13165957227787506,
          0.1316148685128536,
          0.1315693715028147,
          0.13152302357457474,
          0.13147576945755343,
          0.13142755671916434,
          0.13137833614311317,
          0.1313280620390762,
          0.1312766924881171,
          0.1312241895408945,
          0.13117051939258006,
          0.13111565255792593,
          0.1310595640625581,
          0.13100223365590866,
          0.13094364604436615,
          0.1308837911492303,
          0.1308226644192662,
          0.130760267270011,
          0.13069660776660627,
          0.130631701681582,
          0.1305655739884155,
          0.1304982606369809,
          0.13042981013305088,
          0.130360284249763,
          0.13028975742939541,
          0.13021831502694856,
          0.13014605103026988,
          0.13007306592061407,
          0.12999946503712634,
          0.12992535749516093,
          0.1298508555442751,
          0.12977607421890774,
          0.1297011311598583,
          0.1296261465171797,
          0.12955124286646078,
          0.1294765450796535,
          0.1294021800923928,
          0.1293282765045921,
          0.12925496393919705,
          0.1291823720609454,
          0.12911062911353421,
          0.12903985975713428,
          0.12897018189891807,
          0.12890170229966763,
          0.12883451149059075,
          0.1287686799935596,
          0.12870425817029266,
          0.12864127951972093,
          0.12857976472684468,
          0.12851972453991933,
          0.12846116157808496,
          0.12840407176643426,
          0.12834844569483922,
          0.1282942698844865,
          0.12824152790973115,
          0.12819020137427461,
          0.1281402707781171,
          0.12809171633319055,
          0.1280445188059333,
          0.12799866046889968,
          0.127954126116965,
          0.12791090345253875,
          0.12786898025265397,
          0.12782833372106,
          0.12778891323563385,
          0.1277506337682741,
          0.12771339321665343,
          0.12767709620279044,
          0.12764166499503515,
          0.12760703946497193,
          0.12757317340244045,
          0.1275400305160866,
          0.12750758096303164,
          0.12747579853930666,
          0.127444658508778,
          0.12741413605374485,
          0.12738420537491058,
          0.127354839456641,
          0.12732601040471855,
          0.12729769012363104,
          0.12726985104294447,
          0.12724246667112052,
          0.1272155118900175,
          0.12718896301757496,
          0.1271627977203653,
          0.12713699486223518,
          0.12711153435673528,
          0.12708639706857036,
          0.12706156479023037,
          0.1270370203042874,
          0.12701274752727362,
          0.126988731716132,
          0.12696495970385036,
          0.12694142012037257,
          0.1269181035526073,
          0.1268950026061956,
          0.12687211185066008,
          0.12684942765325496,
          0.12682694792772248,
          0.12680467183615343,
          0.12678259948324827,
          0.1267607316347008,
          0.12673906947960034,
          0.12671761444493734,
          0.12669636806115392,
          0.12667533187205254,
          0.12665450737984646,
          0.1266338960157486,
          0.12661349912724854,
          0.12659331797434153,
          0.12657335372793938,
          0.12655360746422134,
          0.12653408014867806,
          0.12651477260308996,
          0.1264956854478496,
          0.12647681901125674,
          0.12645817319730662,
          0.12643974730494306,
          0.12642153979578682,
          0.12640354801480203,
          0.12638576787923317,
          0.12636819356395387,
          0.12635081722294927,
          0.12633362879292864,
          0.1263166159229131,
          0.12629976406283933,
          0.1262830567278866,
          0.12626647593800558,
          0.12625000281718643,
          0.126233618324274,
          0.1262173040742351,
          0.12620104319395664,
          0.1261848211415788,
          0.12616862640861295,
          0.12615245102669792,
          0.12613629082014605,
          0.12612014537970823,
          0.12610401777420094,
          0.126087914053651,
          0.12607184262106297,
          0.12605581355580275,
          0.12603983796188356,
          0.12602392739497578,
          0.12600809339944333,
          0.12599234716636476,
          0.12597669930816166,
          0.1259611597357847,
          0.12594573761966024,
          0.1259304414144371,
          0.1259152789286233,
          0.12590025742234806,
          0.12588538371895025,
          0.12587066431842237,
          0.12585610550273868,
          0.12584171342479278,
          0.12582749417425987,
          0.1258134538155275,
          0.125799598395322,
          0.12578593392115658,
          0.1257724663163404,
          0.1257592013625894,
          0.12574614464614658,
          0.125733301526062,
          0.12572067714218219,
          0.12570827647458094,
          0.1256961044563363,
          0.12568416613009836,
          0.12567246682900238,
          0.125661012356899,
          0.1256498091428071,
          0.12563886434938837,
          0.12562818592331965,
          0.1256177825845655,
          0.1256076637599423,
          0.12559783947288844,
          0.12558832020551933,
          0.1255791167508105,
          0.12557024007231726,
          0.12556170118654836,
          0.12555351107935386,
          0.12554568066291372,
          0.12553822077460802,
          0.12553114221370315,
          0.12552445580686628,
          0.12551817248941333,
          0.12551230338619612,
          0.12550685987433619,
          0.1255018536097252,
          0.12549729650037536,
          0.12549320061232044,
          0.1254895779977731,
          0.12548644044050544,
          0.12548379911970065,
          0.125481664200446,
          0.12548004436606255,
          0.12547894631386802,
          0.1254783742408908,
          0.1254783293486148,
          0.12547880939541628,
          0.1254798083218096,
          0.12548131596749962,
          0.1254833178917249,
          0.12548579530107665,
          0.12548872508365147,
          0.12549207994660955,
          0.1254958286569778,
          0.12549993639272733,
          0.12550436522080222,
          0.12550907472647488,
          0.1255140228174621,
          0.12551916670950508,
          0.12552446406408813,
          0.12552987419902503,
          0.12553535924555062,
          0.1255408851043397,
          0.12554642207378894,
          0.12555194508393575,
          0.12555743354597868,
          0.12556287089253587,
          0.12556824392078475,
          0.12557354205884527,
          0.12557875666283724,
          0.12558388042492286,
          0.12558890693684477,
          0.12559383041651742,
          0.1255986455762499,
          0.12560334759607078,
          0.12560793216404917,
          0.12561239555244935,
          0.12561673470810927,
          0.12562094734349247,
          0.12562503201984265,
          0.12562898821613264,
          0.12563281637835466,
          0.1256365179444967,
          0.12564009534210846,
          0.12564355195778365,
          0.1256468920806992,
          0.12565012082487892,
          0.1256532440365165,
          0.1256562681932626,
          0.12565920030193467,
          0.1256620477999622,
          0.12566481846439526,
          0.1256675203308168,
          0.12567016162320951,
          0.12567275069484274,
          0.12567529597957072,
          0.12567780595253095,
          0.125680289099035,
          0.12568275389039107,
          0.12568520876543451,
          0.12568766211662688,
          0.12569012227969437,
          0.12569259752589382,
          0.1256950960561164,
          0.1256976259961604,
          0.12570019539262878,
          0.12570281220902715,
          0.12570548432175047,
          0.1257082195157361,
          0.12571102547960353,
          0.12571390980006883,
          0.1257168799552849,
          0.125719943306486,
          0.12572310708690804,
          0.12572637838643952,
          0.12572976412992437,
          0.12573327104664261,
          0.1257369056284689,
          0.12574067407481138,
          0.12574458222388726,
          0.1257486354722403,
          0.1257528386873965,
          0.12575719612155634,
          0.12576171133633351,
          0.12576638714888266,
          0.12577122560784584,
          0.12577622800364524,
          0.1257813949127098,
          0.12578672627054835,
          0.1257922214652881,
          0.12579787944192328,
          0.12580369880794542,
          0.1258096779326251,
          0.12581581503417474,
          0.12582210825059673,
          0.12582855569069593,
          0.12583515546134436,
          0.1258419056658503,
          0.12584880436697832,
          0.12585584950817066,
          0.1258630387897613,
          0.12587036950523608,
          0.12587783835605093,
          0.12588544127884257,
          0.12589317332856653,
          0.12590102865638197,
          0.1259090005983612,
          0.12591708185740316,
          0.12592526473185467,
          0.1259335413345844,
          0.12594190375851488,
          0.1259503441696322,
          0.125958854832875,
          0.12596742809107875,
          0.12597605632064413,
          0.12598473188311082,
          0.12599344708392352,
          0.12600219414185326,
          0.12601096516653665,
          0.12601975213781944,
          0.12602854687870546,
          0.12603734101313813,
          0.12604612590008385,
          0.12605489253620292,
          0.12606363142098737,
          0.12607233238147147,
          0.12608098436031048,
          0.12608957518432767,
          0.12609809135490177,
          0.12610651794051891,
          0.12611483870230575,
          0.1261230366226313,
          0.12613109497931346,
          0.12613899893996586,
          0.12614673733454343,
          0.12615430397144234,
          0.12616169789375092,
          0.12616892242852934,
          0.12617598344690695,
          0.1261828875013577,
          0.12618964035415947,
          0.1261962460943815,
          0.1262027067934104,
          0.12620902253948502,
          0.12621519167981343,
          0.12622121112937196,
          0.12622707664792,
          0.12623278303408736,
          0.12623832424112696,
          0.12624369348535547,
          0.1262488834866367,
          0.12625388702058457,
          0.1262586979186741,
          0.12626331246740788,
          0.12626773084878468,
          0.12627195800603702,
          0.12627600336948375,
          0.12627987930938064,
          0.1262835987289595,
          0.12628717251163785,
          0.12629060747595758,
          0.12629390520459174,
          0.1262970617828571,
          0.1263000682319384
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "index"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line({'SGD':losses})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
